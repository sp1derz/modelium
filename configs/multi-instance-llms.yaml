# Modelium Config: Multi-Instance (LLMs Focus)
# Perfect for: Companies serving many LLMs at scale
# Deploy this on a powerful GPU server (4-8 GPUs)

organization:
  id: "llm-company"
  name: "LLM Company Inc"
  enable_usage_tracking: true

# Force all LLMs to use vLLM
runtime:
  default: "auto"
  overrides:
    llm: "vllm"

gpu:
  enabled: true
  count: 4  # This server has 4 GPUs

vllm:
  enabled: true
  tensor_parallel_size: 2  # Use 2 GPUs per model for large LLMs
  gpu_memory_utilization: 0.9
  quantization: "awq"  # Use AWQ quantization for efficiency
  port: 8000

# Workload separation: Dedicated for LLMs only
workload_separation:
  enabled: true
  instances:
    llm_instance:
      description: "Dedicated LLM serving"
      model_types: ["llm", "language_model", "causal_lm"]
      runtime: "vllm"
      gpu_count: 4
      port_offset: 0

# S3 storage for production
storage:
  models_dir: "s3://llm-company-models/incoming"
  backend: "s3"
  s3:
    bucket: "llm-company-models"
    region: "us-west-2"

# Production monitoring
monitoring:
  enabled: true
  prometheus:
    enabled: true
    port: 9090
  grafana:
    enabled: true
    port: 3000

# Usage tracking for billing
usage_tracking:
  enabled: true
  track_inference_calls: true
  track_gpu_hours: true
  export_to: "prometheus"

# Rate limiting per organization
rate_limiting:
  enabled: true
  per_organization:
    enabled: true
    default_rpm: 5000
    overrides:
      "premium-customer-123": 50000
      "enterprise-customer-456": 100000

