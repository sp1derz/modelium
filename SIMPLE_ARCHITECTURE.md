# ğŸ¯ Modelium - Simple Architecture

## The Vision (Your Words)
> **"Watch folder â†’ Drop model â†’ Brain decides runtime â†’ Load it â†’ Track metrics â†’ Unload if idle"**

## The Reality (What We Built)

```
User drops model in /models/incoming/
           â†“
    ModelWatcher detects it
           â†“
    Orchestrator.on_model_discovered()
           â†“
    HuggingFaceAnalyzer reads config.json
           â†“
    Brain decides: vLLM? Triton? Ray?
           â†“
    RuntimeManager.load_model(runtime=chosen)
           â†“
    Inference available at /predict/{model}
           â†“
    Prometheus tracks QPS, latency, idle time
           â†“
    If idle > threshold: RuntimeManager.unload_model()
           â†“
    GPU freed for next model!
```

## Directory Structure (SIMPLE)

```
modelium/
â”œâ”€â”€ runtime_manager.py        â† ONE file for ALL runtimes (vLLM/Triton/Ray)
â”œâ”€â”€ brain/                    
â”‚   â””â”€â”€ unified_brain.py      â† LLM decision maker
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ orchestrator.py       â† Watches â†’ Decides â†’ Loads
â”‚   â”œâ”€â”€ model_watcher.py      â† Monitors /models/incoming/
â”‚   â””â”€â”€ model_registry.py     â† Tracks what's loaded
â”œâ”€â”€ metrics/
â”‚   â””â”€â”€ prometheus_exporter.py â† Tracks everything
â”œâ”€â”€ core/analyzers/
â”‚   â””â”€â”€ huggingface_analyzer.py â† Reads config.json
â””â”€â”€ cli.py                    â† python -m modelium.cli serve
```

**That's it!** No connectors, no managers, no repository - just 7 core files.

## User Configuration (SIMPLE)

```yaml
# modelium.yaml - ONLY WHAT USERS NEED TO CONFIGURE

# Which runtimes to use? (Enable what you have)
vllm:
  enabled: true          # â† Set to true if you want to use vLLM
triton:
  enabled: false
ray_serve:
  enabled: false

# Watch this folder for new models
orchestration:
  model_discovery:
    watch_directories: 
      - /models/incoming  # â† Drop models here

# When to unload idle models?
  policies:
    evict_after_idle_seconds: 300  # â† 5 minutes idle = unload

# Metrics
metrics:
  enabled: true
  port: 9090  # â† Prometheus at http://localhost:9090/metrics
```

**That's all the user needs to configure!** Everything else is automatic.

## How It Works (THE CORE)

### 1. RuntimeManager (ONE FILE - 450 lines)

```python
# modelium/runtime_manager.py

class RuntimeManager:
    """Handles vLLM, Triton, AND Ray in one place."""
    
    def load_model(self, model_name, model_path, runtime, gpu_id):
        """
        Load model into specified runtime.
        
        - vLLM: Spawns process (vllm.entrypoints.openai.api_server)
        - Triton: Calls /v2/repository/models/{name}/load API
        - Ray: serve.run(deployment)
        """
        if runtime == "vllm":
            return self._load_vllm(...)
        elif runtime == "triton":
            return self._load_triton(...)
        elif runtime == "ray":
            return self._load_ray(...)
    
    def unload_model(self, model_name):
        """
        Unload model from its runtime.
        
        - vLLM: Kill process
        - Triton: /v2/repository/models/{name}/unload
        - Ray: serve.delete()
        """
    
    def inference(self, model_name, prompt, **kwargs):
        """Route inference to correct runtime automatically."""
```

**Why ONE file?**
- No confusion: Everything related to runtimes is in ONE place
- Easy to understand: Read 450 lines, you know how EVERYTHING works
- Easy to extend: Want to add TGI? Add `_load_tgi()` method. Done.

### 2. Orchestrator (THE BRAIN)

```python
# modelium/services/orchestrator.py

class Orchestrator:
    """Watches â†’ Analyzes â†’ Decides â†’ Loads."""
    
    def on_model_discovered(self, model_name, model_path):
        """Called when watcher detects new model."""
        
        # 1. ANALYZE: Read config.json
        analysis = self.analyzer.analyze(model_path)
        
        # 2. BRAIN DECIDES: Which runtime?
        runtime = self._choose_runtime(analysis)  # GPT? vLLM. Vision? Ray.
        
        # 3. LOAD: Use RuntimeManager
        self.runtime_manager.load_model(
            model_name=model_name,
            model_path=model_path,
            runtime=runtime,
            gpu_id=self._choose_gpu()  # Pick least used GPU
        )
        
        # 4. DONE! Model is now loaded and ready.
    
    def _check_for_idle_models(self):
        """Background loop: Unload models idle > 5 minutes."""
        for model in self.registry.get_loaded_models():
            if model.idle_seconds > threshold:
                self.runtime_manager.unload_model(model.name)
```

**Why THIS design?**
- Clear flow: 1 â†’ 2 â†’ 3 â†’ 4, no magic
- Brain makes ONE decision: Which runtime?
- RuntimeManager handles the rest

### 3. Metrics (PROMETHEUS)

```python
# modelium/metrics/prometheus_exporter.py

class ModeliumMetrics:
    """Track everything that matters."""
    
    def record_request(self, model, runtime, latency_ms, status, gpu):
        """Track each inference request."""
        # QPS, latency, errors
    
    def get_model_idle_seconds(self, model, runtime):
        """How long since last request?"""
        # Used by orchestrator to decide when to unload
```

**What's tracked?**
- Requests per second (QPS)
- Latency (P50, P95, P99)
- Idle time (for unload decisions)
- GPU memory (optional)
- Brain decisions (load/unload reasons)

## The Flow (VISUAL)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User: cp gpt2/ /models/incoming/                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   ModelWatcher        â”‚  (Scans /models/incoming/ every 30s)
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Orchestrator        â”‚  
         â”‚  .on_model_discovered â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  HuggingFaceAnalyzer  â”‚  (Read config.json â†’ GPT2)
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Brain               â”‚  (GPT2 = LLM â†’ vLLM is best)
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  RuntimeManager       â”‚  
         â”‚  .load_model(vllm)    â”‚  (Spawns: vllm --model /models/incoming/gpt2)
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  vLLM Process         â”‚  (Listening on http://localhost:8100)
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Modelium Server      â”‚  
         â”‚  /predict/gpt2        â”‚  (Routes to vLLM)
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  User: curl POST      â”‚  
         â”‚  /predict/gpt2        â”‚  (Inference!)
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Prometheus Metrics   â”‚  (Tracks QPS, latency, idle time)
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“ (5 minutes of no requests)
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Orchestrator         â”‚  (Idle detected â†’ Unload)
         â”‚  .unload_model(gpt2)  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  RuntimeManager       â”‚  (Kill vLLM process)
         â”‚  .unload_model()      â”‚  (GPU freed!)
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Why This is SIMPLE

### Before (COMPLEX):
```
modelium/
â”œâ”€â”€ connectors/        â† 4 files (800 lines) - HTTP clients
â”‚   â”œâ”€â”€ vllm_connector.py
â”‚   â”œâ”€â”€ triton_connector.py
â”‚   â””â”€â”€ ray_connector.py
â”œâ”€â”€ managers/          â† 4 files (1300 lines) - Process managers
â”‚   â”œâ”€â”€ vllm_manager.py
â”‚   â”œâ”€â”€ triton_manager.py
â”‚   â””â”€â”€ ray_manager.py
â””â”€â”€ repository/        â† 2 files (400 lines) - File restructuring
    â””â”€â”€ model_repository.py

Total: 10 files, ~2,500 lines
```

**Problem**: Where do I look to understand how vLLM loading works?
- `vllm_connector.py`? 
- `vllm_manager.py`? 
- Both? 
- What's the difference?

### After (SIMPLE):
```
modelium/
â””â”€â”€ runtime_manager.py  â† 1 file (450 lines) - EVERYTHING

Total: 1 file, 450 lines
```

**Solution**: ONE place. Read `runtime_manager.py`, understand EVERYTHING.

## File Size Comparison

| Component | Before | After | Change |
|-----------|--------|-------|--------|
| Runtime handling | 2,500 lines (10 files) | 450 lines (1 file) | **-82%** |
| Directories | 6 (services, connectors, managers, repository, metrics, brain) | 4 (services, metrics, brain, core) | **-33%** |
| User confusion | High | **Zero** | âœ… |

## What Users See (THE EXPERIENCE)

### Step 1: Install
```bash
git clone https://github.com/sp1derz/modelium
cd modelium
pip install -e ".[all]"
```

### Step 2: Configure (ONE FILE)
```bash
nano modelium.yaml
# Set vllm.enabled: true
# Done!
```

### Step 3: Start
```bash
python -m modelium.cli serve
# ğŸ§  Modelium Server starting...
# âœ… Server ready at http://0.0.0.0:8000
```

### Step 4: Drop Model
```bash
cp -r my-gpt2-model /models/incoming/
# Server logs:
# ğŸ“‹ New model discovered: my-gpt2-model
# ğŸ¯ Brain decision: vllm
# ğŸš€ Loading model...
# âœ… my-gpt2-model loaded successfully!
```

### Step 5: Use It
```bash
curl http://localhost:8000/predict/my-gpt2-model \
  -d '{"prompt": "Hello", "max_tokens": 50}'
```

### Step 6: Metrics
```bash
# http://localhost:9090/metrics
# modelium_requests_total{model="my-gpt2-model",runtime="vllm"} 1
# modelium_latency_seconds{model="my-gpt2-model"} 0.123
```

### Step 7: Automatic Unload
```
# (After 5 minutes of no requests)
# Server logs:
# ğŸ”½ Unloading idle model: my-gpt2-model (idle: 300s, QPS: 0.00)
# âœ… GPU freed!
```

## Summary

**THE GOAL**: Maximum GPU utilization with minimum user effort

**THE SOLUTION**: 
- Watch folder
- Analyze model (config.json)
- Brain decides runtime (vLLM/Triton/Ray)
- Load automatically
- Track metrics (Prometheus)
- Unload idle models
- **ALL IN 7 FILES**

**NO MORE**:
- âŒ Separate connectors directory
- âŒ Separate managers directory
- âŒ Separate repository directory
- âŒ Confusion about what goes where

**JUST**:
- âœ… `runtime_manager.py` - Handles ALL runtimes
- âœ… `orchestrator.py` - Makes decisions
- âœ… `model_watcher.py` - Watches folder
- âœ… `prometheus_exporter.py` - Tracks metrics
- âœ… `unified_brain.py` - Chooses runtime

**USER EXPERIENCE**:
1. Enable runtimes in `modelium.yaml`
2. Drop models in `/models/incoming/`
3. That's it!

**The complexity is GONE. The functionality remains.**

