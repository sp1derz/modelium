"""
vLLM runtime for serving large language models.

vLLM provides high-throughput and memory-efficient inference for LLMs.
Best for: Llama, Mistral, Qwen, GPT-style models, etc.
"""

import logging
from pathlib import Path
from typing import Dict, Any, List, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class VLLMConfig:
    """Configuration for vLLM deployment."""
    
    model_path: str
    model_name: str
    tensor_parallel_size: int = 1  # Number of GPUs for tensor parallelism
    dtype: str = "auto"  # auto, float16, bfloat16, float32
    max_model_len: Optional[int] = None  # Maximum sequence length
    gpu_memory_utilization: float = 0.9  # GPU memory utilization
    port: int = 8000
    host: str = "0.0.0.0"
    trust_remote_code: bool = True  # For HuggingFace models
    enable_lora: bool = False  # Enable LoRA adapters
    max_loras: int = 1
    quantization: Optional[str] = None  # awq, gptq, squeezellm
    

class VLLMDeployment:
    """
    Manages vLLM deployments for LLM inference.
    
    vLLM is optimized for:
    - High throughput LLM serving
    - PagedAttention for efficient memory usage
    - Continuous batching
    - Fast decoding with various optimizations
    
    Usage:
        deployment = VLLMDeployment()
        config = VLLMConfig(
            model_path="meta-llama/Llama-2-7b-hf",
            model_name="llama2-7b"
        )
        deployment.deploy(config)
    """
    
    def __init__(self) -> None:
        self.logger = logging.getLogger(self.__class__.__name__)
        self._process: Optional[Any] = None
    
    def deploy(self, config: VLLMConfig) -> Dict[str, Any]:
        """
        Deploy a model with vLLM.
        
        Args:
            config: VLLMConfig with deployment settings
            
        Returns:
            Deployment info including endpoint URL
        """
        self.logger.info(f"Deploying model {config.model_name} with vLLM")
        
        # Check if vLLM is installed
        try:
            import vllm
            from vllm import LLM, SamplingParams
            from vllm.entrypoints.openai.api_server import run_server
        except ImportError:
            raise ImportError(
                "vLLM is not installed. Install with: pip install vllm"
            )
        
        # Build vLLM command for OpenAI-compatible API server
        cmd = self._build_command(config)
        
        self.logger.info(f"vLLM command: {' '.join(cmd)}")
        
        return {
            "model_name": config.model_name,
            "endpoint": f"http://{config.host}:{config.port}/v1",
            "engine": "vllm",
            "command": cmd,
            "openai_compatible": True,
            "docs": f"http://{config.host}:{config.port}/docs"
        }
    
    def _build_command(self, config: VLLMConfig) -> List[str]:
        """Build vLLM server command."""
        cmd = [
            "python", "-m", "vllm.entrypoints.openai.api_server",
            "--model", config.model_path,
            "--host", config.host,
            "--port", str(config.port),
            "--tensor-parallel-size", str(config.tensor_parallel_size),
            "--dtype", config.dtype,
            "--gpu-memory-utilization", str(config.gpu_memory_utilization),
        ]
        
        if config.trust_remote_code:
            cmd.append("--trust-remote-code")
        
        if config.max_model_len:
            cmd.extend(["--max-model-len", str(config.max_model_len)])
        
        if config.quantization:
            cmd.extend(["--quantization", config.quantization])
        
        if config.enable_lora:
            cmd.append("--enable-lora")
            cmd.extend(["--max-loras", str(config.max_loras)])
        
        return cmd
    
    def generate_deployment_script(self, config: VLLMConfig) -> str:
        """
        Generate a standalone deployment script.
        
        Returns:
            Shell script to run vLLM server
        """
        cmd = self._build_command(config)
        
        script = f"""#!/bin/bash
# vLLM Deployment Script for {config.model_name}
# Generated by Modelium

set -e

echo "ðŸš€ Starting vLLM server for {config.model_name}..."
echo "ðŸ“ Endpoint: http://{config.host}:{config.port}/v1"
echo "ðŸ“š Docs: http://{config.host}:{config.port}/docs"
echo ""

# Check if vLLM is installed
if ! python -c "import vllm" 2>/dev/null; then
    echo "âŒ vLLM not installed"
    echo "ðŸ“¦ Installing vLLM..."
    pip install vllm
fi

# Run vLLM server
{' '.join(cmd)}
"""
        
        return script
    
    def generate_docker_compose(self, config: VLLMConfig) -> str:
        """
        Generate docker-compose.yml for vLLM deployment.
        
        Returns:
            Docker compose configuration
        """
        compose = f"""version: '3.8'

services:
  vllm-{config.model_name}:
    image: vllm/vllm-openai:latest
    container_name: vllm-{config.model_name}
    ports:
      - "{config.port}:{config.port}"
    environment:
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./models:/models
    command:
      - --model
      - {config.model_path}
      - --host
      - {config.host}
      - --port
      - "{config.port}"
      - --tensor-parallel-size
      - "{config.tensor_parallel_size}"
      - --dtype
      - {config.dtype}
      - --gpu-memory-utilization
      - "{config.gpu_memory_utilization}"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: {config.tensor_parallel_size}
              capabilities: [gpu]
    restart: unless-stopped
    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:{config.port}/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

# Usage:
# docker-compose up -d
# 
# Test:
# curl http://localhost:{config.port}/v1/models
#
# Generate:
# curl http://localhost:{config.port}/v1/completions \\
#   -H "Content-Type: application/json" \\
#   -d '{{
#     "model": "{config.model_path}",
#     "prompt": "Hello, world!",
#     "max_tokens": 100
#   }}'
"""
        
        return compose
    
    def generate_kubernetes_manifest(self, config: VLLMConfig) -> str:
        """
        Generate Kubernetes deployment manifest.
        
        Returns:
            Kubernetes YAML manifest
        """
        manifest = f"""apiVersion: v1
kind: Service
metadata:
  name: vllm-{config.model_name}
  namespace: modelium
spec:
  selector:
    app: vllm-{config.model_name}
  ports:
    - port: 80
      targetPort: {config.port}
      protocol: TCP
  type: LoadBalancer

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-{config.model_name}
  namespace: modelium
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-{config.model_name}
  template:
    metadata:
      labels:
        app: vllm-{config.model_name}
    spec:
      containers:
      - name: vllm
        image: vllm/vllm-openai:latest
        command:
          - python
          - -m
          - vllm.entrypoints.openai.api_server
          - --model
          - {config.model_path}
          - --host
          - "0.0.0.0"
          - --port
          - "{config.port}"
          - --tensor-parallel-size
          - "{config.tensor_parallel_size}"
          - --dtype
          - {config.dtype}
        ports:
        - containerPort: {config.port}
        resources:
          requests:
            nvidia.com/gpu: {config.tensor_parallel_size}
            memory: "32Gi"
            cpu: "8"
          limits:
            nvidia.com/gpu: {config.tensor_parallel_size}
            memory: "64Gi"
            cpu: "16"
        volumeMounts:
        - name: cache
          mountPath: /root/.cache/huggingface
        - name: shm
          mountPath: /dev/shm
        readinessProbe:
          httpGet:
            path: /health
            port: {config.port}
          initialDelaySeconds: 60
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /health
            port: {config.port}
          initialDelaySeconds: 120
          periodSeconds: 30
      volumes:
      - name: cache
        emptyDir: {{}}
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: "10Gi"
      nodeSelector:
        cloud.google.com/gke-accelerator: nvidia-tesla-a100  # Adjust for your cloud
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule

# Apply:
# kubectl apply -f vllm-deployment.yaml
#
# Test:
# kubectl port-forward svc/vllm-{config.model_name} 8000:80
# curl http://localhost:8000/v1/models
"""
        
        return manifest


def create_vllm_config_from_descriptor(
    descriptor: Dict[str, Any],
    organizationId: str,
    model_path: str,
) -> VLLMConfig:
    """
    Create vLLM config from model descriptor.
    
    Args:
        descriptor: Model descriptor from analyzer
        organizationId: Organization ID for multi-tenancy
        model_path: Path to model files
        
    Returns:
        VLLMConfig optimized for the model
    """
    # Determine optimal settings based on model
    model_name = descriptor.get("name", "model")
    
    # Auto-detect GPU memory and set tensor parallelism
    try:
        import torch
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            
            # Simple heuristic: use all GPUs for large models
            model_size_gb = descriptor.get("resources", {}).get("memory_bytes", 0) / 1e9
            
            if model_size_gb > 40 and gpu_count > 1:
                tensor_parallel_size = min(gpu_count, 4)
            else:
                tensor_parallel_size = 1
        else:
            tensor_parallel_size = 1
    except:
        tensor_parallel_size = 1
    
    # Determine dtype
    dtype = "auto"
    if "fp16" in model_name.lower():
        dtype = "float16"
    elif "bf16" in model_name.lower() or "bfloat16" in model_name.lower():
        dtype = "bfloat16"
    
    # Check for quantization
    quantization = None
    if "awq" in model_name.lower():
        quantization = "awq"
    elif "gptq" in model_name.lower():
        quantization = "gptq"
    
    return VLLMConfig(
        model_path=model_path,
        model_name=f"{organizationId}-{model_name}",
        tensor_parallel_size=tensor_parallel_size,
        dtype=dtype,
        quantization=quantization,
        trust_remote_code=True,
    )

