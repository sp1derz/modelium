"""
Ray Serve runtime for serving ML models.

Ray Serve provides scalable model serving with auto-scaling, multi-model support,
and works on CPU/GPU. Best for general ML models, computer vision, etc.
"""

import logging
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class RayServeConfig:
    """Configuration for Ray Serve deployment."""
    
    model_path: str
    model_name: str
    model_type: str  # pytorch, onnx, tensorflow
    num_replicas: int = 1  # Number of model replicas
    num_cpus: float = 1.0  # CPUs per replica
    num_gpus: float = 0.0  # GPUs per replica
    max_concurrent_queries: int = 100
    port: int = 8000
    route_prefix: str = "/predict"
    autoscaling: bool = True
    min_replicas: int = 1
    max_replicas: int = 10
    target_num_ongoing_requests_per_replica: int = 5


class RayServeDeployment:
    """
    Manages Ray Serve deployments for ML model inference.
    
    Ray Serve features:
    - Model composition and chaining
    - Auto-scaling based on load
    - Multi-model serving
    - Works on CPU and GPU
    - Built-in batching
    
    Usage:
        deployment = RayServeDeployment()
        config = RayServeConfig(
            model_path="model.onnx",
            model_name="my-model",
            model_type="onnx"
        )
        deployment.deploy(config)
    """
    
    def __init__(self) -> None:
        self.logger = logging.getLogger(self.__class__.__name__)
    
    def deploy(self, config: RayServeConfig) -> Dict[str, Any]:
        """
        Deploy a model with Ray Serve.
        
        Args:
            config: RayServeConfig with deployment settings
            
        Returns:
            Deployment info including endpoint URL
        """
        self.logger.info(f"Deploying model {config.model_name} with Ray Serve")
        
        # Check if Ray is installed
        try:
            import ray
            from ray import serve
        except ImportError:
            raise ImportError(
                "Ray Serve is not installed. Install with: pip install 'ray[serve]'"
            )
        
        # Generate deployment code
        deployment_code = self._generate_deployment_code(config)
        
        return {
            "model_name": config.model_name,
            "endpoint": f"http://localhost:{config.port}{config.route_prefix}",
            "engine": "ray_serve",
            "deployment_code": deployment_code,
            "replicas": f"{config.min_replicas}-{config.max_replicas}" if config.autoscaling else str(config.num_replicas),
        }
    
    def _generate_deployment_code(self, config: RayServeConfig) -> str:
        """Generate Ray Serve deployment code."""
        
        if config.model_type == "pytorch":
            return self._generate_pytorch_deployment(config)
        elif config.model_type == "onnx":
            return self._generate_onnx_deployment(config)
        elif config.model_type == "tensorflow":
            return self._generate_tensorflow_deployment(config)
        else:
            raise ValueError(f"Unsupported model type: {config.model_type}")
    
    def _generate_pytorch_deployment(self, config: RayServeConfig) -> str:
        """Generate PyTorch model deployment."""
        
        code = f'''"""
Ray Serve deployment for PyTorch model: {config.model_name}
Generated by Modelium
"""

import ray
from ray import serve
import torch
import numpy as np
from fastapi import FastAPI
from pydantic import BaseModel
from typing import List

app = FastAPI()


class PredictionRequest(BaseModel):
    """Prediction request schema."""
    input: List[List[float]]


class PredictionResponse(BaseModel):
    """Prediction response schema."""
    output: List[List[float]]


@serve.deployment(
    num_replicas={config.num_replicas if not config.autoscaling else config.min_replicas},
    ray_actor_options={{
        "num_cpus": {config.num_cpus},
        "num_gpus": {config.num_gpus},
    }},
    max_concurrent_queries={config.max_concurrent_queries},
    autoscaling_config={{
        "min_replicas": {config.min_replicas},
        "max_replicas": {config.max_replicas},
        "target_num_ongoing_requests_per_replica": {config.target_num_ongoing_requests_per_replica},
    }} if {config.autoscaling} else None,
)
@serve.ingress(app)
class {config.model_name.replace("-", "_").title()}Model:
    """PyTorch model deployment."""
    
    def __init__(self):
        # Load model
        self.device = torch.device("cuda" if torch.cuda.is_available() and {config.num_gpus} > 0 else "cpu")
        self.model = torch.load("{config.model_path}", map_location=self.device, weights_only=False)
        self.model.eval()
    
    @app.post("{config.route_prefix}", response_model=PredictionResponse)
    async def predict(self, request: PredictionRequest):
        """Make prediction."""
        # Convert input to tensor
        input_tensor = torch.tensor(request.input, dtype=torch.float32).to(self.device)
        
        # Run inference
        with torch.no_grad():
            output = self.model(input_tensor)
        
        # Convert output to list
        output_list = output.cpu().numpy().tolist()
        
        return PredictionResponse(output=output_list)
    
    @app.get("/health")
    async def health(self):
        """Health check."""
        return {{"status": "healthy", "model": "{config.model_name}"}}


# Deployment
def deploy():
    """Deploy the model."""
    ray.init(ignore_reinit_error=True)
    serve.run({config.model_name.replace("-", "_").title()}Model.bind(), name="{config.model_name}", route_prefix="{config.route_prefix}")
    print(f"✅ Model deployed at http://localhost:{config.port}{config.route_prefix}")


if __name__ == "__main__":
    deploy()
'''
        
        return code
    
    def _generate_onnx_deployment(self, config: RayServeConfig) -> str:
        """Generate ONNX model deployment."""
        
        code = f'''"""
Ray Serve deployment for ONNX model: {config.model_name}
Generated by Modelium
"""

import ray
from ray import serve
import onnxruntime as ort
import numpy as np
from fastapi import FastAPI
from pydantic import BaseModel
from typing import List, Dict

app = FastAPI()


class PredictionRequest(BaseModel):
    """Prediction request schema."""
    input: List[List[float]]


class PredictionResponse(BaseModel):
    """Prediction response schema."""
    output: List[List[float]]


@serve.deployment(
    num_replicas={config.num_replicas if not config.autoscaling else config.min_replicas},
    ray_actor_options={{
        "num_cpus": {config.num_cpus},
        "num_gpus": {config.num_gpus},
    }},
    max_concurrent_queries={config.max_concurrent_queries},
    autoscaling_config={{
        "min_replicas": {config.min_replicas},
        "max_replicas": {config.max_replicas},
        "target_num_ongoing_requests_per_replica": {config.target_num_ongoing_requests_per_replica},
    }} if {config.autoscaling} else None,
)
@serve.ingress(app)
class {config.model_name.replace("-", "_").title()}Model:
    """ONNX model deployment."""
    
    def __init__(self):
        # Configure ONNX Runtime
        providers = []
        if {config.num_gpus} > 0:
            providers.append('CUDAExecutionProvider')
        providers.append('CPUExecutionProvider')
        
        # Load model
        self.session = ort.InferenceSession("{config.model_path}", providers=providers)
        
        # Get input/output names
        self.input_name = self.session.get_inputs()[0].name
        self.output_name = self.session.get_outputs()[0].name
    
    @app.post("{config.route_prefix}", response_model=PredictionResponse)
    async def predict(self, request: PredictionRequest):
        """Make prediction."""
        # Convert input to numpy array
        input_array = np.array(request.input, dtype=np.float32)
        
        # Run inference
        output = self.session.run([self.output_name], {{self.input_name: input_array}})
        
        # Convert output to list
        output_list = output[0].tolist()
        
        return PredictionResponse(output=output_list)
    
    @app.get("/health")
    async def health(self):
        """Health check."""
        return {{"status": "healthy", "model": "{config.model_name}"}}


# Deployment
def deploy():
    """Deploy the model."""
    ray.init(ignore_reinit_error=True)
    serve.run({config.model_name.replace("-", "_").title()}Model.bind(), name="{config.model_name}", route_prefix="{config.route_prefix}")
    print(f"✅ Model deployed at http://localhost:{config.port}{config.route_prefix}")


if __name__ == "__main__":
    deploy()
'''
        
        return code
    
    def _generate_tensorflow_deployment(self, config: RayServeConfig) -> str:
        """Generate TensorFlow model deployment."""
        
        code = f'''"""
Ray Serve deployment for TensorFlow model: {config.model_name}
Generated by Modelium
"""

import ray
from ray import serve
import tensorflow as tf
import numpy as np
from fastapi import FastAPI
from pydantic import BaseModel
from typing import List

app = FastAPI()


class PredictionRequest(BaseModel):
    """Prediction request schema."""
    input: List[List[float]]


class PredictionResponse(BaseModel):
    """Prediction response schema."""
    output: List[List[float]]


@serve.deployment(
    num_replicas={config.num_replicas if not config.autoscaling else config.min_replicas},
    ray_actor_options={{
        "num_cpus": {config.num_cpus},
        "num_gpus": {config.num_gpus},
    }},
    max_concurrent_queries={config.max_concurrent_queries},
    autoscaling_config={{
        "min_replicas": {config.min_replicas},
        "max_replicas": {config.max_replicas},
        "target_num_ongoing_requests_per_replica": {config.target_num_ongoing_requests_per_replica},
    }} if {config.autoscaling} else None,
)
@serve.ingress(app)
class {config.model_name.replace("-", "_").title()}Model:
    """TensorFlow model deployment."""
    
    def __init__(self):
        # Configure GPU
        if {config.num_gpus} > 0:
            gpus = tf.config.list_physical_devices('GPU')
            if gpus:
                try:
                    tf.config.set_visible_devices(gpus[0], 'GPU')
                    tf.config.experimental.set_memory_growth(gpus[0], True)
                except RuntimeError as e:
                    print(f"GPU config error: {{e}}")
        
        # Load model
        self.model = tf.saved_model.load("{config.model_path}")
    
    @app.post("{config.route_prefix}", response_model=PredictionResponse)
    async def predict(self, request: PredictionRequest):
        """Make prediction."""
        # Convert input to tensor
        input_tensor = tf.constant(request.input, dtype=tf.float32)
        
        # Run inference
        output = self.model(input_tensor)
        
        # Convert output to list
        output_list = output.numpy().tolist()
        
        return PredictionResponse(output=output_list)
    
    @app.get("/health")
    async def health(self):
        """Health check."""
        return {{"status": "healthy", "model": "{config.model_name}"}}


# Deployment
def deploy():
    """Deploy the model."""
    ray.init(ignore_reinit_error=True)
    serve.run({config.model_name.replace("-", "_").title()}Model.bind(), name="{config.model_name}", route_prefix="{config.route_prefix}")
    print(f"✅ Model deployed at http://localhost:{config.port}{config.route_prefix}")


if __name__ == "__main__":
    deploy()
'''
        
        return code
    
    def generate_deployment_script(self, config: RayServeConfig) -> str:
        """
        Generate standalone deployment script.
        
        Returns:
            Python script to deploy with Ray Serve
        """
        deployment_code = self._generate_deployment_code(config)
        
        return deployment_code


def create_ray_config_from_descriptor(
    descriptor: Dict[str, Any],
    organizationId: str,
    model_path: str,
) -> RayServeConfig:
    """
    Create Ray Serve config from model descriptor.
    
    Args:
        descriptor: Model descriptor from analyzer
        organizationId: Organization ID for multi-tenancy
        model_path: Path to model files
        
    Returns:
        RayServeConfig optimized for the model
    """
    model_name = descriptor.get("name", "model")
    framework = descriptor.get("framework", "unknown")
    
    # Map framework to model type
    model_type_map = {
        "pytorch": "pytorch",
        "onnx": "onnx",
        "tensorflow": "tensorflow",
    }
    model_type = model_type_map.get(framework.lower(), "pytorch")
    
    # Estimate resource needs
    model_size_gb = descriptor.get("resources", {}).get("memory_bytes", 0) / 1e9
    
    # Heuristic: use GPU if model > 100MB
    num_gpus = 1.0 if model_size_gb > 0.1 else 0.0
    
    # Auto-scaling config based on model size
    if model_size_gb < 1:  # Small models
        min_replicas, max_replicas = 1, 10
    elif model_size_gb < 5:  # Medium models
        min_replicas, max_replicas = 1, 5
    else:  # Large models
        min_replicas, max_replicas = 1, 3
    
    return RayServeConfig(
        model_path=model_path,
        model_name=f"{organizationId}-{model_name}",
        model_type=model_type,
        num_gpus=num_gpus,
        num_cpus=2.0,
        autoscaling=True,
        min_replicas=min_replicas,
        max_replicas=max_replicas,
    )

