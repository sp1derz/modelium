"""
Schemas for conversion plans generated by Modelium LLM.

The Modelium LLM takes a ModelDescriptor as input and outputs a ConversionPlan.
"""

from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field


class TargetFormat(str, Enum):
    """Supported target formats for conversion."""
    
    ONNX = "onnx"
    TORCHSCRIPT = "torchscript"
    TENSORRT_FP32 = "tensorrt_fp32"
    TENSORRT_FP16 = "tensorrt_fp16"
    TENSORRT_INT8 = "tensorrt_int8"
    TRT_LLM = "trt_llm"
    VLLM = "vllm"
    OPENVINO = "openvino"


class OptimizationStrategy(str, Enum):
    """Optimization strategies."""
    
    NONE = "none"
    QUANTIZATION = "quantization"
    PRUNING = "pruning"
    DISTILLATION = "distillation"
    GRAPH_OPTIMIZATION = "graph_optimization"
    KERNEL_FUSION = "kernel_fusion"


class BatchingStrategy(str, Enum):
    """Batching strategies for deployment."""
    
    NONE = "none"
    STATIC = "static"
    DYNAMIC = "dynamic"
    CONTINUOUS = "continuous"


class ConversionStep(BaseModel):
    """A single step in the conversion process."""
    
    name: str
    description: str
    command: Optional[str] = None
    script: Optional[str] = None
    dependencies: List[str] = Field(default_factory=list)
    timeout: int = 300  # seconds
    retry_on_failure: bool = False


class TritonConfig(BaseModel):
    """Configuration for Triton Inference Server."""
    
    name: str
    platform: str  # pytorch_libtorch, onnxruntime_onnx, tensorrt_plan
    max_batch_size: int = 8
    input_specs: List[Dict[str, Any]] = Field(default_factory=list)
    output_specs: List[Dict[str, Any]] = Field(default_factory=list)
    dynamic_batching: Optional[Dict[str, Any]] = None
    instance_group: List[Dict[str, Any]] = Field(default_factory=list)
    optimization: Optional[Dict[str, Any]] = None


class KServeManifest(BaseModel):
    """KServe InferenceService manifest."""
    
    name: str
    namespace: str = "modelium-inference"
    predictor: Dict[str, Any]
    resources: Dict[str, Any] = Field(default_factory=dict)
    autoscaling: Optional[Dict[str, Any]] = None


class SmokeTest(BaseModel):
    """Smoke test specification."""
    
    name: str
    description: str
    input_data: Dict[str, Any]
    expected_output_shape: Optional[List[int]] = None
    tolerance: float = 0.001
    script: Optional[str] = None


class FallbackPlan(BaseModel):
    """Fallback plan if primary conversion fails."""
    
    reason: str
    alternative_format: TargetFormat
    steps: List[ConversionStep] = Field(default_factory=list)


class ConversionPlan(BaseModel):
    """
    Complete conversion plan generated by Modelium LLM.
    
    This is the primary output of the Modelium LLM and input to the executor.
    """
    
    # Identification
    plan_id: str
    model_id: str
    created_at: datetime = Field(default_factory=datetime.utcnow)
    
    # Target configuration
    target_format: TargetFormat
    optimization_strategy: OptimizationStrategy = OptimizationStrategy.NONE
    batching_strategy: BatchingStrategy = BatchingStrategy.DYNAMIC
    
    # Conversion steps
    steps: List[ConversionStep] = Field(default_factory=list)
    
    # Deployment configuration
    triton_config: Optional[TritonConfig] = None
    kserve_manifest: Optional[KServeManifest] = None
    
    # Testing
    smoke_tests: List[SmokeTest] = Field(default_factory=list)
    
    # Resource requirements
    required_memory_gb: float = 8.0
    required_gpu_memory_gb: Optional[float] = None
    requires_gpu: bool = True
    estimated_conversion_time_minutes: int = 10
    
    # Fallback
    fallback_plan: Optional[FallbackPlan] = None
    
    # Additional metadata
    notes: List[str] = Field(default_factory=list)
    warnings: List[str] = Field(default_factory=list)
    
    class Config:
        json_schema_extra = {
            "example": {
                "plan_id": "plan-abc123",
                "model_id": "resnet50-v1",
                "target_format": "tensorrt_fp16",
                "optimization_strategy": "graph_optimization",
                "batching_strategy": "dynamic",
                "steps": [
                    {
                        "name": "export_onnx",
                        "description": "Export PyTorch model to ONNX",
                        "command": "python export_onnx.py",
                    }
                ],
            }
        }


class PlanGenerator:
    """Helper class for generating conversion plans."""
    
    @staticmethod
    def create_pytorch_to_tensorrt_plan(
        model_id: str,
        plan_id: str,
        use_fp16: bool = True,
    ) -> ConversionPlan:
        """Create a plan for PyTorch → ONNX → TensorRT conversion."""
        
        target = TargetFormat.TENSORRT_FP16 if use_fp16 else TargetFormat.TENSORRT_FP32
        
        steps = [
            ConversionStep(
                name="export_onnx",
                description="Export PyTorch model to ONNX format",
                script="""
import torch
import torch.onnx

model = torch.load('model.pt')
model.eval()

dummy_input = torch.randn(1, 3, 224, 224)
torch.onnx.export(
    model,
    dummy_input,
    'model.onnx',
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}
)
""",
                timeout=600,
            ),
            ConversionStep(
                name="optimize_onnx",
                description="Optimize ONNX graph",
                command="python -m onnxruntime.transformers.optimizer --input model.onnx --output model_opt.onnx",
                timeout=300,
            ),
            ConversionStep(
                name="convert_tensorrt",
                description="Convert ONNX to TensorRT engine",
                command=f"trtexec --onnx=model_opt.onnx --saveEngine=model.plan {'--fp16' if use_fp16 else ''}",
                timeout=1800,
                retry_on_failure=True,
            ),
        ]
        
        return ConversionPlan(
            plan_id=plan_id,
            model_id=model_id,
            target_format=target,
            optimization_strategy=OptimizationStrategy.GRAPH_OPTIMIZATION,
            batching_strategy=BatchingStrategy.DYNAMIC,
            steps=steps,
            required_gpu_memory_gb=4.0,
            estimated_conversion_time_minutes=15,
        )
    
    @staticmethod
    def create_vllm_deployment_plan(
        model_id: str,
        plan_id: str,
        model_path: str,
        tensor_parallel_size: int = 1,
        quantization: Optional[str] = None,
    ) -> "ConversionPlan":
        """
        Create a deployment plan for vLLM (for LLMs).
        
        Args:
            model_id: ID of the model
            plan_id: ID for this plan
            model_path: Path to model (HuggingFace repo or local path)
            tensor_parallel_size: Number of GPUs for tensor parallelism
            quantization: Quantization method (awq, gptq, etc.)
        """
        steps = []
        
        # If quantization is needed, add conversion step
        if quantization:
            steps.append(
                ConversionStep(
                    name="quantize_model",
                    description=f"Quantize model with {quantization}",
                    script=f"""
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load model
model = AutoModelForCausalLM.from_pretrained("{model_path}")
tokenizer = AutoTokenizer.from_pretrained("{model_path}")

# Quantization will be handled by vLLM at runtime
print("Model ready for vLLM deployment")
""",
                    timeout=600,
                )
            )
        
        # Add vLLM deployment step
        steps.append(
            ConversionStep(
                name="deploy_vllm",
                description="Deploy model with vLLM",
                command=f"python -m vllm.entrypoints.openai.api_server --model {model_path} --tensor-parallel-size {tensor_parallel_size}" + 
                        (f" --quantization {quantization}" if quantization else ""),
                timeout=3600,
                retry_on_failure=True,
            )
        )
        
        return ConversionPlan(
            plan_id=plan_id,
            model_id=model_id,
            target_format=TargetFormat.VLLM,
            optimization_strategy=OptimizationStrategy.QUANTIZATION if quantization else OptimizationStrategy.NONE,
            batching_strategy=BatchingStrategy.CONTINUOUS,  # vLLM uses continuous batching
            steps=steps,
            required_gpu_memory_gb=16.0,  # Typical for 7B model
            estimated_conversion_time_minutes=5,
        )
    
    @staticmethod
    def create_ray_serve_plan(
        model_id: str,
        plan_id: str,
        model_path: str,
        model_type: str = "pytorch",
        convert_to_onnx: bool = True,
    ) -> "ConversionPlan":
        """
        Create a deployment plan for Ray Serve (for general models).
        
        Args:
            model_id: ID of the model
            plan_id: ID for this plan
            model_path: Path to model
            model_type: Type of model (pytorch, onnx, tensorflow)
            convert_to_onnx: Whether to convert PyTorch to ONNX first
        """
        steps = []
        
        # If PyTorch and conversion requested, convert to ONNX
        if model_type == "pytorch" and convert_to_onnx:
            steps.append(
                ConversionStep(
                    name="export_onnx",
                    description="Export PyTorch model to ONNX for better performance",
                    script=f"""
import torch
import torch.onnx

# Load model
model = torch.load("{model_path}", map_location="cpu", weights_only=False)
model.eval()

# Create dummy input (adjust shape as needed)
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "model.onnx",
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={{'input': {{0: 'batch_size'}}, 'output': {{0: 'batch_size'}}}}
)

print("✅ Exported to ONNX: model.onnx")
""",
                    timeout=300,
                )
            )
        
        # Add Ray Serve deployment step
        steps.append(
            ConversionStep(
                name="deploy_ray_serve",
                description="Deploy model with Ray Serve",
                command="python deploy_ray.py",  # This will be generated
                timeout=600,
                retry_on_failure=True,
                dependencies=["export_onnx"] if (model_type == "pytorch" and convert_to_onnx) else [],
            )
        )
        
        target_format = TargetFormat.ONNX if convert_to_onnx else TargetFormat.TORCHSCRIPT
        
        return ConversionPlan(
            plan_id=plan_id,
            model_id=model_id,
            target_format=target_format,
            optimization_strategy=OptimizationStrategy.GRAPH_OPTIMIZATION,
            batching_strategy=BatchingStrategy.DYNAMIC,
            steps=steps,
            required_gpu_memory_gb=4.0,
            estimated_conversion_time_minutes=5,
        )
    
    @staticmethod
    def choose_best_runtime(
        model_descriptor: Dict[str, Any],
        available_gpus: int = 0,
    ) -> str:
        """
        Intelligently choose the best runtime based on model characteristics.
        
        Args:
            model_descriptor: Model descriptor from analyzer
            available_gpus: Number of available GPUs
            
        Returns:
            Runtime name: 'vllm', 'ray_serve', 'tensorrt', or 'triton'
        """
        model_type = model_descriptor.get("model_type", "unknown").lower()
        framework = model_descriptor.get("framework", "unknown").lower()
        model_size_gb = model_descriptor.get("resources", {}).get("memory_bytes", 0) / 1e9
        
        # Detect if it's an LLM
        is_llm = (
            model_type in ["language_model", "llm", "causal_lm"] or
            "llm" in model_descriptor.get("name", "").lower() or
            "llama" in model_descriptor.get("name", "").lower() or
            "gpt" in model_descriptor.get("name", "").lower() or
            "mistral" in model_descriptor.get("name", "").lower() or
            model_size_gb > 5  # Large models are likely LLMs
        )
        
        # Decision tree
        if is_llm and available_gpus > 0:
            # Use vLLM for LLMs with GPU
            return "vllm"
        elif framework == "pytorch" and available_gpus > 0 and model_size_gb < 2:
            # Small PyTorch models -> TensorRT for max performance
            return "tensorrt"
        elif available_gpus > 0:
            # General models with GPU -> Ray Serve with GPU
            return "ray_serve"
        else:
            # CPU-only -> Ray Serve (works on CPU)
            return "ray_serve"

