# Default values for modelium Helm chart

# Global settings
global:
  imageRegistry: ghcr.io
  imageTag: latest
  imagePullPolicy: Always

# Image configuration
image:
  repository: sp1derz/modelium
  pullPolicy: Always
  tag: ""  # Overrides global.imageTag

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

# Replica count (keep at 1 for GPU)
replicaCount: 1

# Service account
serviceAccount:
  create: true
  annotations: {}
  name: "modelium-server"

# Pod annotations
podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "9090"
  prometheus.io/path: "/metrics"

# Pod security context
podSecurityContext:
  fsGroup: 1000
  runAsNonRoot: true
  runAsUser: 1000

# Container security context
securityContext:
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem: false
  allowPrivilegeEscalation: false

# Service configuration
service:
  type: ClusterIP
  port: 8000
  metricsPort: 9090
  annotations: {}

# Ingress configuration
ingress:
  enabled: true
  className: nginx
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/proxy-body-size: "100m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
  hosts:
    - host: modelium.example.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: modelium-tls
      hosts:
        - modelium.example.com

# Resource requests and limits
resources:
  requests:
    memory: "16Gi"
    cpu: "4"
    nvidia.com/gpu: "4"
  limits:
    memory: "64Gi"
    cpu: "16"
    nvidia.com/gpu: "4"

# Node selector for GPU nodes
nodeSelector:
  nvidia.com/gpu: "true"

# Tolerations for GPU taints
tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule

# Affinity rules
affinity: {}

# Persistent storage for models
persistence:
  models:
    enabled: true
    storageClass: "fast-ssd"
    accessMode: ReadWriteMany
    size: 500Gi
  logs:
    enabled: true
    storageClass: "standard"
    accessMode: ReadWriteOnce
    size: 50Gi

# Modelium configuration
config:
  organization:
    id: "production-cluster"
    name: "Production Cluster"
  
  gpu:
    enabled: true
    count: null  # Auto-detect
  
  modelium_brain:
    enabled: true
    model_name: "Qwen/Qwen2.5-1.5B-Instruct"
    device: "cuda:0"
    fallback_to_rules: true
  
  orchestration:
    enabled: true
    mode: "intelligent"
    decision_interval_seconds: 10
    model_discovery:
      watch_directories:
        - "/models/incoming"
      scan_interval_seconds: 30
    policies:
      evict_after_idle_seconds: 300
      evict_when_memory_above_percent: 85
      max_concurrent_loads: 2
  
  vllm:
    enabled: true
    gpu_memory_utilization: 0.9
    port: 8000
  
  ray_serve:
    enabled: true
    num_gpus_per_replica: 1.0
    port: 8001
  
  metrics:
    enabled: true
    port: 9090

# Probes configuration
livenessProbe:
  httpGet:
    path: /health
    port: http
  initialDelaySeconds: 60
  periodSeconds: 30
  timeoutSeconds: 10
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /health
    port: http
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

startupProbe:
  httpGet:
    path: /health
    port: http
  initialDelaySeconds: 10
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 30

# Environment variables
env:
  - name: LOG_LEVEL
    value: INFO
  - name: PYTHONUNBUFFERED
    value: "1"

# Additional environment from secrets/configmaps
envFrom: []

# Autoscaling (disabled for GPU workloads)
autoscaling:
  enabled: false
